import pandas as pd

# Assuming you have the dataframes loaded as df_participants, df_engagements, df_demographics, and df_events

# Merging the dataframes on 'ParticipantID'
df_merged = df_participants.merge(df_engagements, on='ParticipantID', how='inner')\
                           .merge(df_demographics, on='ParticipantID', how='inner')\
                           .merge(df_events, on='ParticipantID', how='inner')

# Display the first few rows of the merged dataframe
print(df_merged.head())

----------------------------------------------------------

import pandas as pd

# Assuming df_interventions is already defined and loaded with data

# Merging df_merged with df_interventions
df_final = df_merged.merge(df_interventions, on='ParticipantID', how='left')

# Display the first few rows of the final merged dataframe
print(df_final.head())

-----------------------------------------------------------
import pandas as pd
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample DataFrame
data = {'InterventionDetailsAndObjectivesDescription': ['The goal is to enhance job skills', 'Increase employment rate among graduates', 'Provide vocational training and support']}
df = pd.DataFrame(data)

# Text preprocessing
df['cleaned'] = df['InterventionDetailsAndObjectivesDescription'].str.lower().str.replace(r'\W', ' ')

# Tokenization
df['tokens'] = df['cleaned'].apply(nltk.word_tokenize)

# TF-IDF for keyword extraction
vectorizer = TfidfVectorizer(max_features=5)  # Adjust number of features
tfidf_matrix = vectorizer.fit_transform(df['cleaned'])
feature_names = vectorizer.get_feature_names_out()

print("Top keywords based on TF-IDF:")
print(feature_names)
-------------------------------------------------------------
import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Make sure to download stopwords
nltk.download('stopwords')
nltk.download('punkt')

# Sample DataFrame
data = {'InterventionDetailsAndObjectivesDescription': ['The goal is to enhance job skills', 'Increase employment rate among graduates', 'Provide vocational training and support']}
df = pd.DataFrame(data)

# Text preprocessing
df['cleaned'] = df['InterventionDetailsAndObjectivesDescription'].str.lower().str.replace(r'\W', ' ').str.replace(r'\d+', ' ')

# Removing stopwords
stop_words = set(stopwords.words('english'))
df['filtered'] = df['cleaned'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))

# TF-IDF for keyword extraction
vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.5, ngram_range=(1,2))  # Adjust parameters as needed
tfidf_matrix = vectorizer.fit_transform(df['filtered'])
feature_names = vectorizer.get_feature_names_out()

print("Top keywords based on TF-IDF:")
print(feature_names)
--------------------------------------------------------
import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize stopwords
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))

# Adding custom stopwords
custom_stopwords = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '2022', '2023', '2024', '22', '23', '24', '30', 'co', 'uk', 'via', 'email']
stop_words.update(custom_stopwords)

# Sample DataFrame
data = {'InterventionDetailsAndObjectivesDescription': ['The goal is to enhance job skills', 'Increase employment rate among graduates', 'Provide vocational training and support']}
df = pd.DataFrame(data)

# Text preprocessing
df['cleaned'] = df['InterventionDetailsAndObjectivesDescription'].str.lower().str.replace(r'\W', ' ').str.replace(r'\d+', ' ')

# Removing stopwords
df['filtered'] = df['cleaned'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))

# TF-IDF for keyword extraction
vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.5, ngram_range=(1,2))  # Adjust parameters as needed
tfidf_matrix = vectorizer.fit_transform(df['filtered'])
feature_names = vectorizer.get_feature_names_out()

print("Top keywords based on TF-IDF:")
print(feature_names)
--------------------------------------------------------------
pip install spacy
python -m spacy download en_core_web_sm

import pandas as pd
import spacy

# Load spaCy's English NLP model
nlp = spacy.load('en_core_web_sm')

# Sample DataFrame (Replace this with your actual DataFrame)
data = {
    'InterventionDetailsAndObjectivesDescription': [
        'The goal is to enhance job skills in New York',
        'Increase employment rate among graduates from Stanford University',
        'Provide vocational training and support in San Francisco'
    ]
}
df = pd.DataFrame(data)

# Function to extract entities
def extract_entities(text):
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
    return entities

# Apply the function to the DataFrame
df['Entities'] = df['InterventionDetailsAndObjectivesDescription'].apply(extract_entities)

print(df['Entities'])


-------------- Merging Dataframes ---------

I have following spark dataframes
engagements_sdf = spark.sql("SELECT * FROM RestartData.FactParticipantEngagements")
demographics_sdf = spark.sql("SELECT * FROM RestartData.FactDemographics")
interventions_sdf = spark.sql("SELECT * FROM RestartData.FactInterventions")
events_sdf = spark.sql("SELECT * FROM RestartData.`Rst_vw_FactKeyEvents`")

all the dataframes have one to one relationship with ParticipantID except for dataframe interventions_sdf where the relation of all other dataframes is 1 to many. I want to merge all dataframes

---------- Column Names for mapped_interventions---------

Column names in interventions_mapped_sdf:
InterventionID
ParticipantID
OrganisationName
InterventionTotalDuration
InterventionStartDate
InterventionDateAgreedDate
InterventionToBeCompletedByDate
InterventionEndDate
InterventionSpecialistCompleteDate
InterventionAddedDate
InterventionCompleteDate
InterventionLastUpdatedDate
InterventionActivityTypeDescription
InterventionDetailsAndObjectivesDescription
InterventionEvidenceCompletionDescription
InterventionLeftEarlyWithdrawnReasonDescription
InterventionMandatedDescription
InterventionOwnerDescription
InterventionPersonalisedActionDescription
InterventionSource1Description
InterventionSource2Description
InterventionSpecialistMethodOfContactDescription
InterventionSpecialistNotesDescription
InterventionSpecialistStatusDescription
InterventionStatusDescription
InterventionTypeDescription
RecordLoadDate
RecordModifiedDate
RecordDataSource
InterventionApprovalStatusDescription
InterventionServiceTitleDescription
InterventionServiceDeliveryMethodDescription
InterventionServiceFundedUnfundedDescription
InterventionServiceAccecptingReferralsDescription
InterventionServiceUnitPrice
InterventionServiceQuota
InterventionServiceQuotaUsed
InterventionPersonalisedAction
ConcatenatedText
AreaOfFocus
Level
Category
MatchingKeywords

--------------------- merging datagrames --------
# Load dataframes (assuming they are already loaded as engagements_sdf, demographics_sdf, events_sdf, interventions_mapped_sdf)

# Step 1: Left join engagements_sdf and demographics_sdf with events_sdf
merged_df = events_sdf.join(engagements_sdf, "ParticipantID", "left") \
                      .join(demographics_sdf, "ParticipantID", "left")

# Step 2: Left join the merged dataframe with interventions_mapped_sdf on ParticipantID to retain all interventions
final_df = merged_df.join(interventions_mapped_sdf, "ParticipantID", "left")

------------- merge dataframes and create encoded columns for modelling --------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count, avg, sum as _sum

# Initialize Spark session
spark = SparkSession.builder.appName("Data Preparation").getOrCreate()

# Load dataframes (assuming they are already loaded as engagements_sdf, demographics_sdf, events_sdf, interventions_mapped_sdf)

# Step 1: Get distinct values of columns
area_of_focus_values = interventions_mapped_sdf.select("AreaOfFocus").distinct().rdd.flatMap(lambda x: x).collect()
level_values = interventions_mapped_sdf.select("Level").distinct().rdd.flatMap(lambda x: x).collect()
category_values = interventions_mapped_sdf.select("Category").distinct().rdd.flatMap(lambda x: x).collect()

# Step 2: Create binary columns for each unique value in AreaOfFocus, Level, and Category
for value in area_of_focus_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"AreaOfFocus_{value}", when(col("AreaOfFocus") == value, 1).otherwise(0))

for value in level_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"Level_{value}", when(col("Level") == value, 1).otherwise(0))

for value in category_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"Category_{value}", when(col("Category") == value, 1).otherwise(0))

# Step 3: Aggregate binary columns to get count for each ParticipantID
agg_exprs = [
    avg("InterventionTotalDuration").alias("AvgInterventionTotalDuration")
] + [
    _sum(col(f"AreaOfFocus_{value}")).alias(f"AreaOfFocus_{value}") for value in area_of_focus_values
] + [
    _sum(col(f"Level_{value}")).alias(f"Level_{value}") for value in level_values
] + [
    _sum(col(f"Category_{value}")).alias(f"Category_{value}") for value in category_values
]

interventions_agg_sdf = interventions_mapped_sdf.groupBy("ParticipantID").agg(*agg_exprs)

# Step 4: Left join engagements_sdf and demographics_sdf with events_sdf to retain all values of events_sdf
merged_df = events_sdf.join(engagements_sdf, "ParticipantID", "left") \
                      .join(demographics_sdf, "ParticipantID", "left")

# Step 5: Left join the merged dataframe with aggregated interventions data on ParticipantID
final_df = merged_df.join(interventions_agg_sdf, "ParticipantID", "left")

# Show the final prepared dataframe
final_df.show()

# If you want to save the final dataframe
# final_df.write.format("csv").option("header", "true").save("/path/to/save/final_dataframe.csv")

--------------- updated aggregation one hot encoding code -------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, avg, sum as _sum

# Initialize Spark session
spark = SparkSession.builder.appName("Data Preparation").getOrCreate()

# Load dataframes (assuming they are already loaded as engagements_sdf, demographics_sdf, events_sdf, interventions_mapped_sdf)

# Step 1: Get distinct values of columns
area_of_focus_values = interventions_mapped_sdf.select("AreaOfFocus").distinct().rdd.flatMap(lambda x: x).collect()
level_values = interventions_mapped_sdf.select("Level").distinct().rdd.flatMap(lambda x: x).collect()
category_values = interventions_mapped_sdf.select("Category").distinct().rdd.flatMap(lambda x: x).collect()

# Step 2: Create binary columns for each unique value in AreaOfFocus, Level, and Category
for value in area_of_focus_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"AreaOfFocus_{value}", when(col("AreaOfFocus") == value, 1).otherwise(0))

for value in level_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"Level_{value}", when(col("Level") == value, 1).otherwise(0))

for value in category_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"Category_{value}", when(col("Category") == value, 1).otherwise(0))

# Step 3: Aggregate binary columns to get count for each ParticipantID
agg_exprs = [
    avg("InterventionTotalDuration").alias("AvgInterventionTotalDuration")
] + [
    _sum(col(f"AreaOfFocus_{value}")).alias(f"AreaOfFocus_{value}") for value in area_of_focus_values
] + [
    _sum(col(f"Level_{value}")).alias(f"Level_{value}") for value in level_values
] + [
    _sum(col(f"Category_{value}")).alias(f"Category_{value}") for value in category_values
]

# Only select ParticipantID and the columns we are aggregating
columns_to_select = ["ParticipantID"] + [f"AreaOfFocus_{value}" for value in area_of_focus_values] + \
                    [f"Level_{value}" for value in level_values] + \
                    [f"Category_{value}" for value in category_values] + ["InterventionTotalDuration"]

interventions_mapped_sdf_selected = interventions_mapped_sdf.select(*columns_to_select)

interventions_agg_sdf = interventions_mapped_sdf_selected.groupBy("ParticipantID").agg(*agg_exprs)

# Step 4: Left join engagements_sdf and demographics_sdf with events_sdf to retain all values of events_sdf
merged_df = events_sdf.join(engagements_sdf, "ParticipantID", "left") \
                      .join(demographics_sdf, "ParticipantID", "left")

# Step 5: Left join the merged dataframe with aggregated interventions data on ParticipantID
final_df = merged_df.join(interventions_agg_sdf.select("ParticipantID", *[agg_col.alias(agg_col) for agg_col in interventions_agg_sdf.columns if agg_col != "ParticipantID"]), "ParticipantID", "left")

# Show the final prepared dataframe
final_df.show()

# If you want to save the final dataframe
# final_df.write.format("csv").option("header", "true").save("/path/to/save/final_dataframe.csv")


----------- encoding using pandas dataframe --------------

from pyspark.sql import SparkSession
import pandas as pd
from pyspark.sql.functions import avg

# Initialize Spark session
spark = SparkSession.builder.appName("Data Preparation").getOrCreate()

# Load dataframes (assuming they are already loaded as engagements_sdf, demographics_sdf, events_sdf, interventions_mapped_sdf)

# Step 1: Convert Spark DataFrame to Pandas DataFrame
interventions_pandas_df = interventions_mapped_sdf.toPandas()

# Step 2: One-Hot Encode the Categorical Columns
interventions_pandas_df = pd.get_dummies(interventions_pandas_df, columns=['AreaOfFocus', 'Level', 'Category'], prefix=['AreaOfFocus', 'Level', 'Category'])

# Step 3: Aggregate the One-Hot Encoded Columns and Numerical Columns
agg_columns = [col for col in interventions_pandas_df.columns if col.startswith('AreaOfFocus_') or col.startswith('Level_') or col.startswith('Category_')]
agg_dict = {col: 'sum' for col in agg_columns}
agg_dict['InterventionTotalDuration'] = 'mean'

interventions_agg_pandas_df = interventions_pandas_df.groupby('ParticipantID').agg(agg_dict).reset_index()

# Step 4: Rename aggregated columns
interventions_agg_pandas_df.rename(columns={'InterventionTotalDuration': 'AvgInterventionTotalDuration'}, inplace=True)

# Step 5: Convert the Pandas DataFrame back to a Spark DataFrame
interventions_agg_spark_df = spark.createDataFrame(interventions_agg_pandas_df)

# Step 6: Left join engagements_sdf and demographics_sdf with events_sdf to retain all values of events_sdf
merged_df = events_sdf.join(engagements_sdf, "ParticipantID", "left") \
                      .join(demographics_sdf, "ParticipantID", "left")

# Step 7: Left join the merged dataframe with aggregated interventions data on ParticipantID
final_df = merged_df.join(interventions_agg_spark_df, "ParticipantID", "left")

# Show the final prepared dataframe
final_df.show()

# If you want to save the final dataframe
# final_df.write.format("csv").option("header", "true").save("/path/to/save/final_dataframe.csv")




