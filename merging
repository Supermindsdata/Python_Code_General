import pandas as pd

# Assuming you have the dataframes loaded as df_participants, df_engagements, df_demographics, and df_events

# Merging the dataframes on 'ParticipantID'
df_merged = df_participants.merge(df_engagements, on='ParticipantID', how='inner')\
                           .merge(df_demographics, on='ParticipantID', how='inner')\
                           .merge(df_events, on='ParticipantID', how='inner')

# Display the first few rows of the merged dataframe
print(df_merged.head())

----------------------------------------------------------

import pandas as pd

# Assuming df_interventions is already defined and loaded with data

# Merging df_merged with df_interventions
df_final = df_merged.merge(df_interventions, on='ParticipantID', how='left')

# Display the first few rows of the final merged dataframe
print(df_final.head())

-----------------------------------------------------------
import pandas as pd
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample DataFrame
data = {'InterventionDetailsAndObjectivesDescription': ['The goal is to enhance job skills', 'Increase employment rate among graduates', 'Provide vocational training and support']}
df = pd.DataFrame(data)

# Text preprocessing
df['cleaned'] = df['InterventionDetailsAndObjectivesDescription'].str.lower().str.replace(r'\W', ' ')

# Tokenization
df['tokens'] = df['cleaned'].apply(nltk.word_tokenize)

# TF-IDF for keyword extraction
vectorizer = TfidfVectorizer(max_features=5)  # Adjust number of features
tfidf_matrix = vectorizer.fit_transform(df['cleaned'])
feature_names = vectorizer.get_feature_names_out()

print("Top keywords based on TF-IDF:")
print(feature_names)
-------------------------------------------------------------
import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Make sure to download stopwords
nltk.download('stopwords')
nltk.download('punkt')

# Sample DataFrame
data = {'InterventionDetailsAndObjectivesDescription': ['The goal is to enhance job skills', 'Increase employment rate among graduates', 'Provide vocational training and support']}
df = pd.DataFrame(data)

# Text preprocessing
df['cleaned'] = df['InterventionDetailsAndObjectivesDescription'].str.lower().str.replace(r'\W', ' ').str.replace(r'\d+', ' ')

# Removing stopwords
stop_words = set(stopwords.words('english'))
df['filtered'] = df['cleaned'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))

# TF-IDF for keyword extraction
vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.5, ngram_range=(1,2))  # Adjust parameters as needed
tfidf_matrix = vectorizer.fit_transform(df['filtered'])
feature_names = vectorizer.get_feature_names_out()

print("Top keywords based on TF-IDF:")
print(feature_names)
--------------------------------------------------------
import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize stopwords
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))

# Adding custom stopwords
custom_stopwords = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '2022', '2023', '2024', '22', '23', '24', '30', 'co', 'uk', 'via', 'email']
stop_words.update(custom_stopwords)

# Sample DataFrame
data = {'InterventionDetailsAndObjectivesDescription': ['The goal is to enhance job skills', 'Increase employment rate among graduates', 'Provide vocational training and support']}
df = pd.DataFrame(data)

# Text preprocessing
df['cleaned'] = df['InterventionDetailsAndObjectivesDescription'].str.lower().str.replace(r'\W', ' ').str.replace(r'\d+', ' ')

# Removing stopwords
df['filtered'] = df['cleaned'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))

# TF-IDF for keyword extraction
vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.5, ngram_range=(1,2))  # Adjust parameters as needed
tfidf_matrix = vectorizer.fit_transform(df['filtered'])
feature_names = vectorizer.get_feature_names_out()

print("Top keywords based on TF-IDF:")
print(feature_names)
--------------------------------------------------------------
pip install spacy
python -m spacy download en_core_web_sm

import pandas as pd
import spacy

# Load spaCy's English NLP model
nlp = spacy.load('en_core_web_sm')

# Sample DataFrame (Replace this with your actual DataFrame)
data = {
    'InterventionDetailsAndObjectivesDescription': [
        'The goal is to enhance job skills in New York',
        'Increase employment rate among graduates from Stanford University',
        'Provide vocational training and support in San Francisco'
    ]
}
df = pd.DataFrame(data)

# Function to extract entities
def extract_entities(text):
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
    return entities

# Apply the function to the DataFrame
df['Entities'] = df['InterventionDetailsAndObjectivesDescription'].apply(extract_entities)

print(df['Entities'])


-------------- Merging Dataframes ---------

I have following spark dataframes
engagements_sdf = spark.sql("SELECT * FROM RestartData.FactParticipantEngagements")
demographics_sdf = spark.sql("SELECT * FROM RestartData.FactDemographics")
interventions_sdf = spark.sql("SELECT * FROM RestartData.FactInterventions")
events_sdf = spark.sql("SELECT * FROM RestartData.`Rst_vw_FactKeyEvents`")

all the dataframes have one to one relationship with ParticipantID except for dataframe interventions_sdf where the relation of all other dataframes is 1 to many. I want to merge all dataframes
