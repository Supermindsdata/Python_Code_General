import pandas as pd

# Assuming you have the dataframes loaded as df_participants, df_engagements, df_demographics, and df_events

# Merging the dataframes on 'ParticipantID'
df_merged = df_participants.merge(df_engagements, on='ParticipantID', how='inner')\
                           .merge(df_demographics, on='ParticipantID', how='inner')\
                           .merge(df_events, on='ParticipantID', how='inner')

# Display the first few rows of the merged dataframe
print(df_merged.head())

----------------------------------------------------------

import pandas as pd

# Assuming df_interventions is already defined and loaded with data

# Merging df_merged with df_interventions
df_final = df_merged.merge(df_interventions, on='ParticipantID', how='left')

# Display the first few rows of the final merged dataframe
print(df_final.head())

-----------------------------------------------------------
import pandas as pd
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample DataFrame
data = {'InterventionDetailsAndObjectivesDescription': ['The goal is to enhance job skills', 'Increase employment rate among graduates', 'Provide vocational training and support']}
df = pd.DataFrame(data)

# Text preprocessing
df['cleaned'] = df['InterventionDetailsAndObjectivesDescription'].str.lower().str.replace(r'\W', ' ')

# Tokenization
df['tokens'] = df['cleaned'].apply(nltk.word_tokenize)

# TF-IDF for keyword extraction
vectorizer = TfidfVectorizer(max_features=5)  # Adjust number of features
tfidf_matrix = vectorizer.fit_transform(df['cleaned'])
feature_names = vectorizer.get_feature_names_out()

print("Top keywords based on TF-IDF:")
print(feature_names)
-------------------------------------------------------------
import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Make sure to download stopwords
nltk.download('stopwords')
nltk.download('punkt')

# Sample DataFrame
data = {'InterventionDetailsAndObjectivesDescription': ['The goal is to enhance job skills', 'Increase employment rate among graduates', 'Provide vocational training and support']}
df = pd.DataFrame(data)

# Text preprocessing
df['cleaned'] = df['InterventionDetailsAndObjectivesDescription'].str.lower().str.replace(r'\W', ' ').str.replace(r'\d+', ' ')

# Removing stopwords
stop_words = set(stopwords.words('english'))
df['filtered'] = df['cleaned'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))

# TF-IDF for keyword extraction
vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.5, ngram_range=(1,2))  # Adjust parameters as needed
tfidf_matrix = vectorizer.fit_transform(df['filtered'])
feature_names = vectorizer.get_feature_names_out()

print("Top keywords based on TF-IDF:")
print(feature_names)
--------------------------------------------------------
import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize stopwords
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))

# Adding custom stopwords
custom_stopwords = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '2022', '2023', '2024', '22', '23', '24', '30', 'co', 'uk', 'via', 'email']
stop_words.update(custom_stopwords)

# Sample DataFrame
data = {'InterventionDetailsAndObjectivesDescription': ['The goal is to enhance job skills', 'Increase employment rate among graduates', 'Provide vocational training and support']}
df = pd.DataFrame(data)

# Text preprocessing
df['cleaned'] = df['InterventionDetailsAndObjectivesDescription'].str.lower().str.replace(r'\W', ' ').str.replace(r'\d+', ' ')

# Removing stopwords
df['filtered'] = df['cleaned'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))

# TF-IDF for keyword extraction
vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.5, ngram_range=(1,2))  # Adjust parameters as needed
tfidf_matrix = vectorizer.fit_transform(df['filtered'])
feature_names = vectorizer.get_feature_names_out()

print("Top keywords based on TF-IDF:")
print(feature_names)
--------------------------------------------------------------
pip install spacy
python -m spacy download en_core_web_sm

import pandas as pd
import spacy

# Load spaCy's English NLP model
nlp = spacy.load('en_core_web_sm')

# Sample DataFrame (Replace this with your actual DataFrame)
data = {
    'InterventionDetailsAndObjectivesDescription': [
        'The goal is to enhance job skills in New York',
        'Increase employment rate among graduates from Stanford University',
        'Provide vocational training and support in San Francisco'
    ]
}
df = pd.DataFrame(data)

# Function to extract entities
def extract_entities(text):
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
    return entities

# Apply the function to the DataFrame
df['Entities'] = df['InterventionDetailsAndObjectivesDescription'].apply(extract_entities)

print(df['Entities'])


-------------- Merging Dataframes ---------

I have following spark dataframes
engagements_sdf = spark.sql("SELECT * FROM RestartData.FactParticipantEngagements")
demographics_sdf = spark.sql("SELECT * FROM RestartData.FactDemographics")
interventions_sdf = spark.sql("SELECT * FROM RestartData.FactInterventions")
events_sdf = spark.sql("SELECT * FROM RestartData.`Rst_vw_FactKeyEvents`")

all the dataframes have one to one relationship with ParticipantID except for dataframe interventions_sdf where the relation of all other dataframes is 1 to many. I want to merge all dataframes

---------- Column Names for mapped_interventions---------

Column names in interventions_mapped_sdf:
InterventionID
ParticipantID
OrganisationName
InterventionTotalDuration
InterventionStartDate
InterventionDateAgreedDate
InterventionToBeCompletedByDate
InterventionEndDate
InterventionSpecialistCompleteDate
InterventionAddedDate
InterventionCompleteDate
InterventionLastUpdatedDate
InterventionActivityTypeDescription
InterventionDetailsAndObjectivesDescription
InterventionEvidenceCompletionDescription
InterventionLeftEarlyWithdrawnReasonDescription
InterventionMandatedDescription
InterventionOwnerDescription
InterventionPersonalisedActionDescription
InterventionSource1Description
InterventionSource2Description
InterventionSpecialistMethodOfContactDescription
InterventionSpecialistNotesDescription
InterventionSpecialistStatusDescription
InterventionStatusDescription
InterventionTypeDescription
RecordLoadDate
RecordModifiedDate
RecordDataSource
InterventionApprovalStatusDescription
InterventionServiceTitleDescription
InterventionServiceDeliveryMethodDescription
InterventionServiceFundedUnfundedDescription
InterventionServiceAccecptingReferralsDescription
InterventionServiceUnitPrice
InterventionServiceQuota
InterventionServiceQuotaUsed
InterventionPersonalisedAction
ConcatenatedText
AreaOfFocus
Level
Category
MatchingKeywords

--------------------- merging datagrames --------
# Load dataframes (assuming they are already loaded as engagements_sdf, demographics_sdf, events_sdf, interventions_mapped_sdf)

# Step 1: Left join engagements_sdf and demographics_sdf with events_sdf
merged_df = events_sdf.join(engagements_sdf, "ParticipantID", "left") \
                      .join(demographics_sdf, "ParticipantID", "left")

# Step 2: Left join the merged dataframe with interventions_mapped_sdf on ParticipantID to retain all interventions
final_df = merged_df.join(interventions_mapped_sdf, "ParticipantID", "left")

------------- merge dataframes and create encoded columns for modelling --------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count, avg, sum as _sum

# Initialize Spark session
spark = SparkSession.builder.appName("Data Preparation").getOrCreate()

# Load dataframes (assuming they are already loaded as engagements_sdf, demographics_sdf, events_sdf, interventions_mapped_sdf)

# Step 1: Get distinct values of columns
area_of_focus_values = interventions_mapped_sdf.select("AreaOfFocus").distinct().rdd.flatMap(lambda x: x).collect()
level_values = interventions_mapped_sdf.select("Level").distinct().rdd.flatMap(lambda x: x).collect()
category_values = interventions_mapped_sdf.select("Category").distinct().rdd.flatMap(lambda x: x).collect()

# Step 2: Create binary columns for each unique value in AreaOfFocus, Level, and Category
for value in area_of_focus_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"AreaOfFocus_{value}", when(col("AreaOfFocus") == value, 1).otherwise(0))

for value in level_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"Level_{value}", when(col("Level") == value, 1).otherwise(0))

for value in category_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"Category_{value}", when(col("Category") == value, 1).otherwise(0))

# Step 3: Aggregate binary columns to get count for each ParticipantID
agg_exprs = [
    avg("InterventionTotalDuration").alias("AvgInterventionTotalDuration")
] + [
    _sum(col(f"AreaOfFocus_{value}")).alias(f"AreaOfFocus_{value}") for value in area_of_focus_values
] + [
    _sum(col(f"Level_{value}")).alias(f"Level_{value}") for value in level_values
] + [
    _sum(col(f"Category_{value}")).alias(f"Category_{value}") for value in category_values
]

interventions_agg_sdf = interventions_mapped_sdf.groupBy("ParticipantID").agg(*agg_exprs)

# Step 4: Left join engagements_sdf and demographics_sdf with events_sdf to retain all values of events_sdf
merged_df = events_sdf.join(engagements_sdf, "ParticipantID", "left") \
                      .join(demographics_sdf, "ParticipantID", "left")

# Step 5: Left join the merged dataframe with aggregated interventions data on ParticipantID
final_df = merged_df.join(interventions_agg_sdf, "ParticipantID", "left")

# Show the final prepared dataframe
final_df.show()

# If you want to save the final dataframe
# final_df.write.format("csv").option("header", "true").save("/path/to/save/final_dataframe.csv")

--------------- updated aggregation one hot encoding code -------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, avg, sum as _sum

# Initialize Spark session
spark = SparkSession.builder.appName("Data Preparation").getOrCreate()

# Load dataframes (assuming they are already loaded as engagements_sdf, demographics_sdf, events_sdf, interventions_mapped_sdf)

# Step 1: Get distinct values of columns
area_of_focus_values = interventions_mapped_sdf.select("AreaOfFocus").distinct().rdd.flatMap(lambda x: x).collect()
level_values = interventions_mapped_sdf.select("Level").distinct().rdd.flatMap(lambda x: x).collect()
category_values = interventions_mapped_sdf.select("Category").distinct().rdd.flatMap(lambda x: x).collect()

# Step 2: Create binary columns for each unique value in AreaOfFocus, Level, and Category
for value in area_of_focus_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"AreaOfFocus_{value}", when(col("AreaOfFocus") == value, 1).otherwise(0))

for value in level_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"Level_{value}", when(col("Level") == value, 1).otherwise(0))

for value in category_values:
    interventions_mapped_sdf = interventions_mapped_sdf.withColumn(f"Category_{value}", when(col("Category") == value, 1).otherwise(0))

# Step 3: Aggregate binary columns to get count for each ParticipantID
agg_exprs = [
    avg("InterventionTotalDuration").alias("AvgInterventionTotalDuration")
] + [
    _sum(col(f"AreaOfFocus_{value}")).alias(f"AreaOfFocus_{value}") for value in area_of_focus_values
] + [
    _sum(col(f"Level_{value}")).alias(f"Level_{value}") for value in level_values
] + [
    _sum(col(f"Category_{value}")).alias(f"Category_{value}") for value in category_values
]

# Only select ParticipantID and the columns we are aggregating
columns_to_select = ["ParticipantID"] + [f"AreaOfFocus_{value}" for value in area_of_focus_values] + \
                    [f"Level_{value}" for value in level_values] + \
                    [f"Category_{value}" for value in category_values] + ["InterventionTotalDuration"]

interventions_mapped_sdf_selected = interventions_mapped_sdf.select(*columns_to_select)

interventions_agg_sdf = interventions_mapped_sdf_selected.groupBy("ParticipantID").agg(*agg_exprs)

# Step 4: Left join engagements_sdf and demographics_sdf with events_sdf to retain all values of events_sdf
merged_df = events_sdf.join(engagements_sdf, "ParticipantID", "left") \
                      .join(demographics_sdf, "ParticipantID", "left")

# Step 5: Left join the merged dataframe with aggregated interventions data on ParticipantID
final_df = merged_df.join(interventions_agg_sdf.select("ParticipantID", *[agg_col.alias(agg_col) for agg_col in interventions_agg_sdf.columns if agg_col != "ParticipantID"]), "ParticipantID", "left")

# Show the final prepared dataframe
final_df.show()

# If you want to save the final dataframe
# final_df.write.format("csv").option("header", "true").save("/path/to/save/final_dataframe.csv")


----------- encoding using pandas dataframe --------------

from pyspark.sql import SparkSession
import pandas as pd
from pyspark.sql.functions import avg

# Initialize Spark session
spark = SparkSession.builder.appName("Data Preparation").getOrCreate()

# Load dataframes (assuming they are already loaded as engagements_sdf, demographics_sdf, events_sdf, interventions_mapped_sdf)

# Step 1: Convert Spark DataFrame to Pandas DataFrame
interventions_pandas_df = interventions_mapped_sdf.toPandas()

# Step 2: One-Hot Encode the Categorical Columns
interventions_pandas_df = pd.get_dummies(interventions_pandas_df, columns=['AreaOfFocus', 'Level', 'Category'], prefix=['AreaOfFocus', 'Level', 'Category'])

# Step 3: Aggregate the One-Hot Encoded Columns and Numerical Columns
agg_columns = [col for col in interventions_pandas_df.columns if col.startswith('AreaOfFocus_') or col.startswith('Level_') or col.startswith('Category_')]
agg_dict = {col: 'sum' for col in agg_columns}
agg_dict['InterventionTotalDuration'] = 'mean'

interventions_agg_pandas_df = interventions_pandas_df.groupby('ParticipantID').agg(agg_dict).reset_index()

# Step 4: Rename aggregated columns
interventions_agg_pandas_df.rename(columns={'InterventionTotalDuration': 'AvgInterventionTotalDuration'}, inplace=True)

# Step 5: Convert the Pandas DataFrame back to a Spark DataFrame
interventions_agg_spark_df = spark.createDataFrame(interventions_agg_pandas_df)

# Step 6: Left join engagements_sdf and demographics_sdf with events_sdf to retain all values of events_sdf
merged_df = events_sdf.join(engagements_sdf, "ParticipantID", "left") \
                      .join(demographics_sdf, "ParticipantID", "left")

# Step 7: Left join the merged dataframe with aggregated interventions data on ParticipantID
final_df = merged_df.join(interventions_agg_spark_df, "ParticipantID", "left")

# Show the final prepared dataframe
final_df.show()

# If you want to save the final dataframe
# final_df.write.format("csv").option("header", "true").save("/path/to/save/final_dataframe.csv")

--------------------------- error --------------------
AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_6193/4226285558.py in ?()
      8 agg_columns = [col for col in interventions_pandas_df.columns if col.startswith('AreaOfFocus_') or col.startswith('Level_') or col.startswith('Category_')]
      9 agg_dict = {col: 'sum' for col in agg_columns}
     10 agg_dict['InterventionTotalDuration'] = 'mean'
     11 
---> 12 interventions_agg_pandas_df = interventions_pandas_df.groupby('ParticipantID').agg(agg_dict).reset_index()
     13 
     14 # Step 4: Rename aggregated columns
     15 interventions_agg_pandas_df.rename(columns={'InterventionTotalDuration': 'AvgInterventionTotalDuration'}, inplace=True)

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/groupby/generic.py in ?(self, func, engine, engine_kwargs, *args, **kwargs)
   1265         relabeling, func, columns, order = reconstruct_func(func, **kwargs)
   1266         func = maybe_mangle_lambdas(func)
   1267 
   1268         op = GroupByApply(self, func, args, kwargs)
-> 1269         result = op.agg()
   1270         if not is_dict_like(func) and result is not None:
   1271             return result
   1272         elif relabeling:

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/apply.py in ?(self)
    159         if isinstance(arg, str):
    160             return self.apply_str()
    161 
    162         if is_dict_like(arg):
--> 163             return self.agg_dict_like()
    164         elif is_list_like(arg):
    165             # we require a list, but not a 'str'
    166             return self.agg_list_like()

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/apply.py in ?(self)
    416                 colg = obj._gotitem(selection, ndim=1)
    417                 results = {key: colg.agg(how) for key, how in arg.items()}
    418             else:
    419                 # key used for column selection and output
--> 420                 results = {
    421                     key: obj._gotitem(key, ndim=1).agg(how) for key, how in arg.items()
    422                 }
    423 

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/apply.py in ?(.0)
    420     def agg_dict_like(self) -> DataFrame | Series:
--> 421         """
    422         Compute aggregation in the case of a dict-like argument.
    423 
    424         Returns

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/groupby/generic.py in ?(self, func, engine, engine_kwargs, *args, **kwargs)
    228             columns, func = validate_func_kwargs(kwargs)
    229             kwargs = {}
    230 
    231         if isinstance(func, str):
--> 232             return getattr(self, func)(*args, **kwargs)
    233 
    234         elif isinstance(func, abc.Iterable):
    235             # Catch instances of lists / tuples

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/groupby/groupby.py in ?(self, numeric_only, min_count, engine, engine_kwargs)
   2259             # If we are grouping on categoricals we want unobserved categories to
   2260             # return zero, rather than the default of NaN which the reindexing in
   2261             # _agg_general() returns. GH #31422
   2262             with com.temp_setattr(self, "observed", True):
-> 2263                 result = self._agg_general(
   2264                     numeric_only=numeric_only,
   2265                     min_count=min_count,
   2266                     alias="sum",

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/groupby/groupby.py in ?(self, numeric_only, min_count, alias, npfunc)
   1418         *,
   1419         alias: str,
   1420         npfunc: Callable,
   1421     ):
-> 1422         result = self._cython_agg_general(
   1423             how=alias,
   1424             alt=npfunc,
   1425             numeric_only=numeric_only,

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/groupby/groupby.py in ?(self, how, alt, numeric_only, min_count, **kwargs)
   1504 
   1505             return result
   1506 
   1507         new_mgr = data.grouped_reduce(array_func)
-> 1508         res = self._wrap_agged_manager(new_mgr)
   1509         out = self._wrap_aggregated_output(res)
   1510         if self.axis == 1:
   1511             out = out.infer_objects(copy=False)

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/groupby/generic.py in ?(self, mgr)
    140     def _wrap_agged_manager(self, mgr: Manager) -> Series:
--> 141         return self.obj._constructor(mgr, name=self.obj.name)

~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/generic.py in ?(self, name)
   5985             and name not in self._accessors
   5986             and self._info_axis._can_hold_identifiers_and_holds_name(name)
   5987         ):
   5988             return self[name]
-> 5989         return object.__getattribute__(self, name)

AttributeError: 'DataFrame' object has no attribute 'name'




