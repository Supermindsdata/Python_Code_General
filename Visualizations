##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model, flag=True):
    """
    model : classifier to predict values of X

    """
    # defining an empty list to store train and test results
    score_list = []

    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)

    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)

    train_recall = metrics.recall_score(y_train, pred_train)
    test_recall = metrics.recall_score(y_test, pred_test)

    train_precision = metrics.precision_score(y_train, pred_train)
    test_precision = metrics.precision_score(y_test, pred_test)

    score_list.extend(
        (
            train_acc,
            test_acc,
            train_recall,
            test_recall,
            train_precision,
            test_precision,
        )
    )

    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True:
        print("Accuracy on training set : ", model.score(X_train, y_train))
        print("Accuracy on test set : ", model.score(X_test, y_test))
        print("Recall on training set : ", metrics.recall_score(y_train, pred_train))
        print("Recall on test set : ", metrics.recall_score(y_test, pred_test))
        print(
            "Precision on training set : ", metrics.precision_score(y_train, pred_train)
        )
        print("Precision on test set : ", metrics.precision_score(y_test, pred_test))

    return score_list  # returning the list with train and test scores


############################################################################################

## Function to create confusion matrix
def make_confusion_matrix(model, y_actual, labels=[1, 0]):
    """
    model : classifier to predict values of X
    y_actual : ground truth

    """
    y_predict = model.predict(X_test)
    cm = metrics.confusion_matrix(y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(
        cm,
        index=[i for i in ["Actual - No", "Actual - Yes"]],
        columns=[i for i in ["Predicted - No", "Predicted - Yes"]],
    )
    group_counts = ["{0:0.0f}".format(value) for value in cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in cm.flatten() / np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
    labels = np.asarray(labels).reshape(2, 2)
    plt.figure(figsize=(10, 7))
    sns.heatmap(df_cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")

#####################################################################################

feature_names = X_train.columns
importances = dtree_tuned.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

################################# Function to print importane for top values #############
def plot_top_values_by_percentage(df, column_name):
    """
    Plots a bar chart showing the top 50 values of a specified column by percentage.
    """
    # Calculate value counts for the specified column
    value_counts_result = df[column_name].value_counts()

    # Calculate percentages
    total_values = len(df)
    top_50_values = value_counts_result.head(50)
    percentages = (top_50_values / total_values) * 100

    # Create a color palette based on the size of the bars
    colors = sns.color_palette("coolwarm", len(top_50_values))

    # Create the bar chart
    plt.figure(figsize=(12, 8))
    bars = plt.barh(top_50_values.index, percentages, color=colors)
    plt.xlabel('Percentage')
    plt.ylabel(column_name)
    plt.title(f'Top 50 {column_name} by Percentage')

    # Add percentage labels on top of bars
    for bar, percentage in zip(bars, percentages):
        plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height() / 2,
                 f'{percentage:.2f}%', va='center', color='black')

    plt.tight_layout()
    plt.show()
########################################

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def plot_top_values_by_percentage(df, first_column, third_column):
    """
    Plots a bar chart showing the values of a specified first column by percentage,
    based on the values in the third column, with grouped bars for each value in the first column.
    """
    # Calculate value counts for the specified first column based on third column values
    grouped_df = df.groupby(third_column)[first_column].value_counts(normalize=True).unstack(fill_value=0)

    # Flatten the DataFrame
    flat_df = grouped_df.stack().reset_index()
    flat_df.columns = [third_column, first_column, 'percentage']

    # Create the color palette
    unique_values_third_col = df[third_column].nunique()
    colors = sns.color_palette("coolwarm", unique_values_third_col)

    # Create the bar chart
    plt.figure(figsize=(14, 10))

    all_bars = []
    labels = []
    for i, value in enumerate(df[third_column].unique()):
        subset = flat_df[flat_df[third_column] == value]
        bar = plt.barh(subset[first_column], subset['percentage'] * 100, color=colors[i], alpha=0.7, label=value)
        all_bars.extend(bar)
        labels.extend(subset[first_column].tolist())

        # Add percentage labels on top of bars
        for bar, percentage in zip(bar, subset['percentage'] * 100):
            plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height() / 2,
                     f'{percentage:.2f}%', va='center', color='black')

    plt.xlabel('Percentage')
    plt.ylabel(first_column)
    plt.title(f'{first_column} Distribution by {third_column}')
    plt.legend(title=third_column)
    plt.tight_layout()
    plt.show()

# Example usage with a DataFrame 'df':
# plot_top_values_by_percentage(df, 'FirstColumnName', 'ThirdColumnName')



